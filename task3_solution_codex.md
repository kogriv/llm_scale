# Task 3 Solution (Codex Variant)

## 1. Анализ текущего решения

Анализ текущего решения
В `solve_task3.py` для каждого шага DP хранится ограниченный фронт позиций (до 50 состояний, в отдельных случаях до 60–150) и используется BFS, который возвращает не более 100 ближайших клеток нужного типа, если явно не снят лимит.

Именно эти эвристики (cap на найденные цели и обрезка фронта состояний) были сознательно введены ради скорости, что отмечено и в отчёте как компромисс между точностью и производительностью.

## 2. Почему мог упасть тест 32

Проблема не прохождения теста 32
Если очередной тип встречается сотни раз, BFS ограниченный `cap = 100` вернёт лишь 100 ближайших ячеек от текущей позиции. Для «толстых» кластеров это будут фактически локальные клетки, а дальние (хотя и потенциально выгодные для будущих шагов) в `next_positions` никогда не попадут.

После этого включается pruning по `MAX_STATES`. Даже если одна из 100 найденных клеток ведёт к перспективному маршруту, она может быть отброшена, если её текущая стоимость чуть выше, чем у других кандидатов, потому что сравнение идёт только по стоимости на текущем шаге, без оглядки на «будущее».

Типичный контрпример: стартовое положение окружено «лёгкими» клетками нужного типа, а ближайший «хороший» представитель (рядом с крупным кластером следующих букв) находится дальше и не попадает в топ-100. В результате оптимальная траектория (идти сразу к дальнему представителю и потом быстро обслужить оставшиеся буквы) недостижима. Такие ситуации становятся особенно вероятными на длинных чередованиях «обильных» типов, где локальные решения сильно влияют на глобальный результат. Это как раз сценарий, от которого автор решения предупреждался в отчёте (эвристики могут «пропустить редкие оптимумы»).

## 3. Идеи по алгоритмическим улучшениям

### 3.1 Полностью детерминированный DP без эвристик — **реализовано**

Для каждого шага можно запускать мульти-источниковый BFS от всех позиций текущего фронта, сохраняя их стоимости в очереди. Поскольку размер сетки ≤ 90 000 клеток, а длина строки ≤ 300, суммарная работа составит порядка 27 млн посещений, что ещё реалистично при аккуратной реализации на deque. Такой подход исключает и `cap`, и `MAX_STATES`.

Эта стратегия воплощена в `solve_task3_codex.py`: алгоритм переходит к точному мульти-источниковому поиску, поддерживая стоимость всех достижимых клеток без отсечений. Реализация использует переиспользуемый буфер расстояний и пропускает повторные BFS для одинаковых букв, что удерживает время работы на приемлемом уровне.

### 3.2 Предварительные BFS по типам букв — **не реализовано**

Можно запустить 26 мульти-источниковых BFS (по одному на букву) и получить минимальные расстояния от любого типа к любой клетке. Тогда переход между соседними буквами строки сводится к выбору лучшей ячейки следующего типа с учётом стоимости предыдущего шага. Остаётся аккуратно хранить минимум по всем потенциальным конечным позициям (например, в виде массива размера 26 или вектора по всем клеткам конкретного типа). Это снимет зависимость от локальных ограничений и позволит быстро сравнивать варианты, особенно при длинных фрагментах из одинаковых букв.

Идея оставлена в резерве: текущий мульти-источниковый DP показал достаточную скорость, и дополнительная предобработка на всю сетку усложнила бы код и потребовала больше памяти. При необходимости ускорения на экстремальных тестах можно вернуться к этой альтернативе.

### 3.3 Смягчение эвристик и добавление нижних оценок — **не реализовано**

Если хочется оставить эвристики, их нужно ослабить. Возможные варианты:

* увеличивать `cap` при большом разнообразии следующих букв (например, если в суффиксе строки встречается много типов, значит нужно шире сканировать текущий тип);
* хранить несколько «дальних» целей дополнительно к ближайшим (например, добавлять в `next_positions` 5–10 клеток с максимальным расстоянием из найденных, чтобы хотя бы некоторые дальние варианты переживали pruning);
* делать pruning не только по текущей стоимости, но и по нижней оценке «стоимость + эвристика на остаток» (манхэттенский минимум до ближайшей ячейки каждого будущего типа).

Эти варианты рассматривались как план «Б», но их реализация не понадобилась: отказ от эвристик полностью устранил проблему, поэтому дополнительная настройка heuristics не внедрялась.

### 3.4 Учёт компонент связности — **не реализовано**

Учёт компонент связности.
Если карта разбита на кластеры одного типа, полезно заранее строить граф «компонента ↔ тип», чтобы понимать, какие переходы вообще возможны, и ранжировать компоненты по расстоянию до других типов. Это может пригодиться для безопасного уменьшения фронта: вместо отдельной клетки хранить компоненту и лучшую стоимость внутри неё.

От идеи отказались из-за сложности сопровождения и отсутствия явной выгоды после внедрения точного мульти-источникового DP.

## 4. Что ещё потестировать

Что ещё потестировать
Уже сейчас есть скрипт `compare_solutions.py`, который запускает оптимизированную и эталонную версии на базовом наборе тестов; на них расхождений нет. Но для ловли редких ошибок этого мало.

Воспользуйтесь `stress_test_random.py`: он генерирует случайные карты с кластерами и сравнивает оба решения. Стоит увеличить количество тестов (например, до 1000) и сохранить первый найденный контрпример — это вероятный кандидат на «тест 32».

Сформируйте направленные сценарии: две большие области одного типа на расстоянии >100, одна из них окружена следующими буквами строки. Такое можно быстро сгенерировать, адаптировав идеи из `generate_stress_tests.py` (там уже есть шаблоны для «обильных» типов и зигзагов).

## 5. Новое решение и результаты тестирования

Новый скрипт `solve_task3_codex.py` реализует точный мульти-источниковый поиск: для каждой буквы формируется очередь всех её клеток, а переходы между буквами выполняются через общее поле расстояний, обновляемое функцией `run_bfs`. Используется очередь `deque`, а массивы NumPy не требуются, что позволяет укладываться во временные ограничения даже на крупных сетках. Дополнительно предусмотрено повторное использование массива расстояний между шагами, что уменьшает количество операций и обращения к памяти.

Для подтверждения корректности и производительности выполнены следующие тесты:

* новые направленные случаи `test_far_cluster.txt` и `test_long_alternating.txt`;
* сравнение с `solve_task3_reference.py` на базовом и расширенном наборах входных данных (включая `test_simple.txt`, где ожидаемый вывод `-1` из-за отсутствия целевой буквы);
* запуск на стресс-наборах `stress_test1.txt` – `stress_test5.txt`, показавший укладывание во временные рамки ≤6–9 секунд.

По итогам прогонов расхождений с эталоном не обнаружено, а производительность остаётся практической для ограничений задачи.

